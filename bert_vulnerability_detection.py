"""
BERT-based Vulnerable Smart Contract Detection
Using the BCCC-VulSCs-2023 dataset

This script implements a BERT-based approach for detecting vulnerable smart contracts.
It fine-tunes a pre-trained BERT model on Solidity smart contract source code to classify
them as secure (0) or vulnerable (1).
"""

import os
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertModel, BertForSequenceClassification, get_linear_schedule_with_warmup
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')


class SmartContractDataset(Dataset):
    """Dataset class for smart contract vulnerability detection"""
    
    def __init__(self, texts, labels, tokenizer, max_length=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        
        # Tokenize the text
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'label': torch.tensor(label, dtype=torch.long)
        }


class BERTVulnerabilityDetector:
    """BERT-based model for smart contract vulnerability detection"""
    
    def __init__(self, model_name='bert-base-uncased', num_labels=2, device=None):
        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.tokenizer = BertTokenizer.from_pretrained(model_name)
        self.model = BertForSequenceClassification.from_pretrained(
            model_name,
            num_labels=num_labels
        ).to(self.device)
        print(f"Using device: {self.device}")
    
    def prepare_data(self, texts, labels, test_size=0.2, val_size=0.1, batch_size=16, max_length=512):
        """Prepare data loaders for training, validation, and testing"""
        
        # Split data into train and test
        X_train, X_test, y_train, y_test = train_test_split(
            texts, labels, test_size=test_size, random_state=42, stratify=labels
        )
        
        # Further split train into train and validation
        X_train, X_val, y_train, y_val = train_test_split(
            X_train, y_train, test_size=val_size, random_state=42, stratify=y_train
        )
        
        print(f"Train samples: {len(X_train)}, Validation samples: {len(X_val)}, Test samples: {len(X_test)}")
        
        # Create datasets
        train_dataset = SmartContractDataset(X_train, y_train, self.tokenizer, max_length)
        val_dataset = SmartContractDataset(X_val, y_val, self.tokenizer, max_length)
        test_dataset = SmartContractDataset(X_test, y_test, self.tokenizer, max_length)
        
        # Create data loaders
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
        
        return train_loader, val_loader, test_loader
    
    def train(self, train_loader, val_loader, epochs=5, learning_rate=2e-5, warmup_steps=0):
        """Train the BERT model"""
        
        # Prepare optimizer and scheduler
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate, eps=1e-8)
        total_steps = len(train_loader) * epochs
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=warmup_steps,
            num_training_steps=total_steps
        )
        
        best_val_accuracy = 0
        
        for epoch in range(epochs):
            print(f"\nEpoch {epoch + 1}/{epochs}")
            print("-" * 50)
            
            # Training phase
            self.model.train()
            train_loss = 0
            train_correct = 0
            train_total = 0
            
            train_pbar = tqdm(train_loader, desc="Training")
            for batch in train_pbar:
                # Move batch to device
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['label'].to(self.device)
                
                # Forward pass
                optimizer.zero_grad()
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )
                
                loss = outputs.loss
                logits = outputs.logits
                
                # Backward pass
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                optimizer.step()
                scheduler.step()
                
                # Calculate accuracy
                predictions = torch.argmax(logits, dim=1)
                train_correct += (predictions == labels).sum().item()
                train_total += labels.size(0)
                train_loss += loss.item()
                
                # Update progress bar
                train_pbar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    'acc': f'{train_correct/train_total:.4f}'
                })
            
            avg_train_loss = train_loss / len(train_loader)
            train_accuracy = train_correct / train_total
            
            # Validation phase
            val_accuracy, val_loss = self.evaluate(val_loader)
            
            print(f"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}")
            print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}")
            
            # Save best model
            if val_accuracy > best_val_accuracy:
                best_val_accuracy = val_accuracy
                torch.save(self.model.state_dict(), 'best_bert_model.pt')
                print(f"Best model saved with validation accuracy: {val_accuracy:.4f}")
    
    def evaluate(self, data_loader):
        """Evaluate the model"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for batch in data_loader:
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['label'].to(self.device)
                
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )
                
                loss = outputs.loss
                logits = outputs.logits
                
                predictions = torch.argmax(logits, dim=1)
                correct += (predictions == labels).sum().item()
                total += labels.size(0)
                total_loss += loss.item()
        
        accuracy = correct / total
        avg_loss = total_loss / len(data_loader)
        
        return accuracy, avg_loss
    
    def predict(self, data_loader):
        """Make predictions on data"""
        self.model.eval()
        all_predictions = []
        all_labels = []
        all_probs = []
        
        with torch.no_grad():
            for batch in tqdm(data_loader, desc="Predicting"):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['label'].to(self.device)
                
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask
                )
                
                logits = outputs.logits
                probs = torch.softmax(logits, dim=1)
                predictions = torch.argmax(logits, dim=1)
                
                all_predictions.extend(predictions.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                all_probs.extend(probs[:, 1].cpu().numpy())  # Probability of class 1 (vulnerable)
        
        return np.array(all_predictions), np.array(all_labels), np.array(all_probs)
    
    def get_metrics(self, y_true, y_pred, y_probs):
        """Calculate and display evaluation metrics"""
        accuracy = accuracy_score(y_true, y_pred)
        precision = precision_score(y_true, y_pred)
        recall = recall_score(y_true, y_pred)
        f1 = f1_score(y_true, y_pred)
        auc = roc_auc_score(y_true, y_probs)
        cm = confusion_matrix(y_true, y_pred)
        
        print("\n" + "="*50)
        print("Model Performance Metrics")
        print("="*50)
        print(f"Accuracy:  {accuracy:.4f}")
        print(f"Precision: {precision:.4f}")
        print(f"Recall:    {recall:.4f}")
        print(f"F1-Score:  {f1:.4f}")
        print(f"ROC-AUC:   {auc:.4f}")
        print("\nConfusion Matrix:")
        print(f"TN: {cm[0, 0]}, FP: {cm[0, 1]}")
        print(f"FN: {cm[1, 0]}, TP: {cm[1, 1]}")
        print("="*50)
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'auc': auc,
            'confusion_matrix': cm
        }


def load_dataset(secure_path='BCCC-VolSCs-2023_Secure_.csv', 
                vulnerable_path='BCCC-VolSCs-2023_Vulnerable.csv',
                source_code_dir=None):
    """
    Load the BCCC-VulSCs-2023 dataset
    
    Args:
        secure_path: Path to secure contracts CSV
        vulnerable_path: Path to vulnerable contracts CSV
        source_code_dir: Optional directory containing source code files
    
    Returns:
        texts: List of contract source codes or bytecode representations
        labels: List of labels (0 for secure, 1 for vulnerable)
    """
    # Load CSV files
    secure_df = pd.read_csv(secure_path)
    vulnerable_df = pd.read_csv(vulnerable_path)
    
    # Combine datasets
    df = pd.concat([secure_df, vulnerable_df], ignore_index=True)
    
    print(f"Total samples: {len(df)}")
    print(f"Secure samples: {len(secure_df)}")
    print(f"Vulnerable samples: {len(vulnerable_df)}")
    
    # Get labels
    labels = df['label'].values
    
    # Load source code if available
    if source_code_dir and os.path.exists(source_code_dir):
        print("Loading source code from files...")
        texts = []
        for hash_id in tqdm(df['hash_id'], desc="Loading contracts"):
            file_path = os.path.join(source_code_dir, f"{hash_id}.sol")
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    texts.append(f.read())
            except:
                # If source file not found, use bytecode features as fallback
                texts.append(create_bytecode_text_representation(df[df['hash_id'] == hash_id]))
    else:
        # Use bytecode features as text representation
        print("Creating text representation from bytecode features...")
        texts = []
        for idx in tqdm(range(len(df)), desc="Processing"):
            texts.append(create_bytecode_text_representation(df.iloc[idx]))
    
    return texts, labels


def create_bytecode_text_representation(row):
    """
    Create a text representation from bytecode features
    This serves as a placeholder when source code is not available
    """
    # Select relevant bytecode features
    bytecode_features = [
        'bytecode_len',
        'Weight bytecode_character_6',
        'Weight bytecode_character_0',
        'Weight bytecode_character_8',
        'Weight bytecode_character_4',
        'Weight bytecode_character_5',
        'Weight bytecode_character_2',
    ]
    
    text_parts = []
    for feature in bytecode_features:
        if feature in row:
            value = row[feature]
            # Create a descriptive text representation
            feature_name = feature.replace('_', ' ').replace('Weight bytecode character', 'char')
            text_parts.append(f"{feature_name}: {value}")
    
    return " | ".join(text_parts)


def main():
    """Main execution function"""
    print("="*50)
    print("BERT-based Vulnerable Smart Contract Detection")
    print("="*50)
    
    # Configuration
    BATCH_SIZE = 16
    EPOCHS = 5
    LEARNING_RATE = 2e-5
    MAX_LENGTH = 512
    
    # Load dataset
    print("\nLoading dataset...")
    texts, labels = load_dataset()
    
    # Initialize BERT detector
    print("\nInitializing BERT model...")
    detector = BERTVulnerabilityDetector(model_name='bert-base-uncased')
    
    # Prepare data loaders
    print("\nPreparing data loaders...")
    train_loader, val_loader, test_loader = detector.prepare_data(
        texts, labels,
        batch_size=BATCH_SIZE,
        max_length=MAX_LENGTH
    )
    
    # Train the model
    print("\nTraining BERT model...")
    detector.train(
        train_loader,
        val_loader,
        epochs=EPOCHS,
        learning_rate=LEARNING_RATE
    )
    
    # Load best model
    print("\nLoading best model...")
    detector.model.load_state_dict(torch.load('best_bert_model.pt'))
    
    # Evaluate on test set
    print("\nEvaluating on test set...")
    y_pred, y_true, y_probs = detector.predict(test_loader)
    metrics = detector.get_metrics(y_true, y_pred, y_probs)
    
    print("\nTraining completed successfully!")
    print(f"Best model saved as 'best_bert_model.pt'")


if __name__ == "__main__":
    main()
