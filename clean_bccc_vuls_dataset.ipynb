{"cells":[{"cell_type":"markdown","metadata":{},"source":["# clean_bccc_vuls_dataset.ipynb\n","Analyze two CSV files (expected to be BCCC-VulSCs-2023 CSVs or similar feature matrices)\n","and produce statistics and charts for:\n","- missing values (per-column and per-row)\n","- null / NaN counts\n","- total samples (per-file and combined)\n","- feature types (dtypes and counts)\n","- duplicates, constant columns, near-zero variance candidates\n","- memory usage and sparsity\n","- numeric distributions, correlations, and top correlated pairs\n","- categorical cardinalities and top categories\n","- basic class/label balance detection (if label-like columns exist)\n","Dependencies:\n","    pandas, numpy, matplotlib, seaborn, scipy (optional)\n","    Install with: pip install pandas numpy matplotlib seaborn scipy\n","Outputs:\n","    - Console summary\n","    - PNG charts saved in the output directory\n","    - summary JSON saved as summary_<timestamp>.json in the output directory"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import sys\n","import json\n","import math\n","from datetime import datetime\n","from collections import Counter\n","\n","try:\n","    import numpy as np\n","    import pandas as pd\n","    import matplotlib.pyplot as plt\n","    import seaborn as sns\n","    from scipy import stats\n","except Exception as e:\n","    print(\"Error importing modules: {}\".format(e), file=sys.stderr)\n","    print(\"Make sure you have pandas, numpy, matplotlib, seaborn and scipy installed.\", file=sys.stderr)\n","    print(\"Install with: pip install pandas numpy matplotlib seaborn scipy\", file=sys.stderr)\n","    raise\n","\n","sns.set(style=\"whitegrid\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def mkdir_p(path):\n","    os.makedirs(path, exist_ok=True)\n","\n","def safe_filename(s):\n","    return \"\".join(c if c.isalnum() or c in \"-_.'\" else \"_\" for c in str(s))\n","\n","def detect_label_columns(df):\n","    candidates = []\n","    lower = [c.lower() for c in df.columns]\n","    for name in df.columns:\n","        n = name.lower()\n","        if n in (\"label\", \"target\", \"class\", \"vulnerable\", \"y\", \"is_vulnerable\"):\n","            candidates.append(name)\n","        elif \"vul\" in n and (\"label\" in n or \"flag\" in n or \"is\" in n):\n","            candidates.append(name)\n","    if not candidates:\n","        for name in df.columns:\n","            nunique = df[name].nunique(dropna=True)\n","            if nunique == 2:\n","                candidates.append(name)\n","    return candidates\n","\n","def summarize_df_basic(df):\n","    summary = {}\n","    summary['num_rows'] = int(df.shape[0])\n","    summary['num_columns'] = int(df.shape[1])\n","    summary['memory_usage_bytes'] = int(df.memory_usage(deep=True).sum())\n","    summary['memory_usage_human'] = f\"{summary['memory_usage_bytes'] / (1024**2):.2f} MB\"\n","    summary['num_duplicates'] = int(df.duplicated().sum())\n","    const_cols = [c for c in df.columns if df[c].nunique(dropna=False) <= 1]\n","    summary['constant_columns'] = const_cols\n","    missing_per_col = df.isna().sum()\n","    summary['columns_missing_count'] = int((missing_per_col > 0).sum())\n","    summary['rows_with_any_missing'] = int(df.isna().any(axis=1).sum())\n","    summary['percent_rows_with_missing'] = float(summary['rows_with_any_missing']) / max(1, summary['num_rows'])\n","    dtypes = df.dtypes.astype(str).value_counts().to_dict()\n","    summary['dtypes'] = dtypes\n","    return summary\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_missing_by_column(df, outpath, title_suffix=\""):\n","    missing = df.isna().sum()\n","    missing = missing[missing > 0].sort_values(ascending=False)\n","    if missing.empty:\n","        print(\"No missing values by column to plot.\")\n","        return None\n","    plt.figure(figsize=(max(6, min(20, 0.2 * len(missing))), 6))\n","    sns.barplot(x=missing.values, y=missing.index, palette=\"viridis\")\n","    plt.xlabel(\"Missing values (count)\")\n","    plt.ylabel(\"Column\")\n","    plt.title(f\"Missing values per column {title_suffix}\")\n","    plt.tight_layout()\n","    plt.savefig(outpath, dpi=150)\n","    plt.close()\n","    return outpath\n","\n","def plot_missing_matrix(df, outpath, title_suffix=\""):\n","    nrows = df.shape[0]\n","    max_rows_for_plot = 1000\n","    if nrows > max_rows_for_plot:\n","        sample_df = df.sample(n=max_rows_for_plot, random_state=0)\n","    else:\n","        sample_df = df\n","    mat = sample_df.isna().T.astype(int)\n","    plt.figure(figsize=(min(12, 0.02 * mat.shape[1] + 6), max(3, 0.02 * mat.shape[0] + 3)))\n","    sns.heatmap(mat, cmap=\"Greys\", cbar=False)\n","    plt.xlabel(\"Sampled row index\")\n","    plt.ylabel(\"Column\")\n","    plt.title(f\"Missingness matrix (sampled up to {max_rows_for_plot} rows) {title_suffix}\")\n","    plt.tight_layout()\n","    plt.savefig(outpath, dpi=150)\n","    plt.close()\n","    return outpath\n","\n","def plot_missing_row_counts(df, outpath, title_suffix=\""):\n","    missing_per_row = df.isna().sum(axis=1)\n","    plt.figure(figsize=(8, 4))\n","    sns.histplot(missing_per_row, bins=50, kde=False)\n","    plt.xlabel(\"Missing values in row\")\n","    plt.ylabel(\"Count of rows\")\n","    plt.title(f\"Distribution of missing values per row {title_suffix}\")\n","    plt.tight_layout()\n","    plt.savefig(outpath, dpi=150)\n","    plt.close()\n","    return outpath\n","\n","def plot_dtype_counts(df, outpath, title_suffix=\""):\n","    dtypes = df.dtypes.astype(str).value_counts()\n","    plt.figure(figsize=(6, 4))\n","    sns.barplot(x=dtypes.index, y=dtypes.values, palette=\"pastel\")\n","    plt.ylabel(\"Number of columns\")\n","    plt.xlabel(\"Dtype\")\n","    plt.title(f\"Feature types {title_suffix}\")\n","    plt.tight_layout()\n","    plt.savefig(outpath, dpi=150)\n","    plt.close()\n","    return outpath\n","\n","def plot_numeric_distributions(df, outdir, prefix, max_cols=12):\n","    numeric = df.select_dtypes(include=[np.number])\n","    if numeric.shape[1] == 0:\n","        return []\n","    variances = numeric.var(numeric_only=True).sort_values(ascending=False)\n","    selected = variances.index[:max_cols].tolist()\n","    outfiles = []\n","    for col in selected:\n","        series = numeric[col].dropna()\n","        if series.shape[0] == 0:\n","            continue\n","        plt.figure(figsize=(6, 4))\n","        sns.histplot(series, bins=50, kde=True, color='C0')\n","        plt.xlabel(col)\n","        plt.title(f\"Distribution of {col}\")\n","        plt.tight_layout()\n","        fname = os.path.join(outdir, f\"{safe_filename(prefix)}_dist_{safe_filename(col)}.png\")\n","        plt.savefig(fname, dpi=150)\n","        plt.close()\n","        outfiles.append(fname)\n","    return outfiles\n","\n","def compute_top_correlations(df, top_k=20):\n","    numeric = df.select_dtypes(include=[np.number])\n","    if numeric.shape[1] < 2:\n","        return []\n","    try:\n","        corr = numeric.corr(method='spearman')\n","    except Exception:\n","        corr = numeric.corr(method='pearson')\n","    corr_pairs = []\n","    cols = corr.columns\n","    for i in range(len(cols)):\n","        for j in range(i+1, len(cols)):\n","            val = corr.iloc[i, j]\n","            corr_pairs.append((cols[i], cols[j], float(val)))\n","    corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n","    return corr_pairs[:top_k], corr\n","\n","def plot_correlation_heatmap(corr_df, outpath, title_suffix=\""):\n","    if corr_df is None or corr_df.size == 0:\n","        return None\n","    n = corr_df.shape[0]\n","    max_n = 40\n","    if n > max_n:\n","        variances = corr_df.var().sort_values(ascending=False)\n","        selected = variances.index[:max_n]\n","        corr_plot = corr_df.loc[selected, selected]\n","    else:\n","        corr_plot = corr_df\n","    plt.figure(figsize=(min(16, 0.4 * corr_plot.shape[0] + 6), min(16, 0.4 * corr_plot.shape[1] + 6)))\n","    sns.heatmap(corr_plot, cmap='vlag', center=0, square=True, linewidths=0.1)\n","    plt.title(f\"Correlation heatmap {title_suffix}\")\n","    plt.tight_layout()\n","    plt.savefig(outpath, dpi=150)\n","    plt.close()\n","    return outpath\n","\n","def analyze_categorical(df, outdir, prefix, max_unique_for_plot=20):\n","    cat_cols = df.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n","    outfiles = []\n","    cat_summary = {}\n","    for col in cat_cols:\n","        nunique = df[col].nunique(dropna=True)\n","        cat_summary[col] = {'nunique': int(nunique)}\n","        if nunique <= max_unique_for_plot:\n","            counts = df[col].value_counts(dropna=False)\n","            plt.figure(figsize=(6, max(3, 0.3 * len(counts))))\n","            sns.barplot(y=counts.index.astype(str), x=counts.values, palette=\"Set3\")\n","            plt.xlabel(\"Count\")\n","            plt.ylabel(col)\n","            plt.title(f\"Value counts for {col}\")\n","            plt.tight_layout()\n","            fname = os.path.join(outdir, f\"{safe_filename(prefix)}_cat_counts_{safe_filename(col)}.png\")\n","            plt.savefig(fname, dpi=150)\n","            plt.close()\n","            outfiles.append(fname)\n","            top = counts.head(10).to_dict()\n","            cat_summary[col]['top_values'] = {str(k): int(v) for k, v in top.items()}\n","    return cat_summary, outfiles\n","\n","def find_near_zero_variance(df, freq_cut=95/5, unique_cut=10):\n","    nzv = []\n","    for col in df.columns:\n","        s = df[col].dropna()\n","        if s.empty:\n","            nzv.append(col)\n","            continue\n","        nunique = s.nunique()\n","        if nunique <= 1:\n","            nzv.append(col)\n","            continue\n","        if pd.api.types.is_numeric_dtype(s):\n","            if s.std() == 0:\n","                nzv.append(col)\n","                continue\n","            if abs(s.mean()) > 0:\n","                cv = s.std() / (abs(s.mean()) + 1e-12)\n","                if cv < 1e-6:\n","                    nzv.append(col)\n","                    continue\n","        top_counts = s.value_counts().values\n","        if len(top_counts) >= 2:\n","            ratio = top_counts[0] / (top_counts[1] + 1e-12)\n","            if ratio > freq_cut:\n","                nzv.append(col)\n","        elif len(top_counts) == 1:\n","            nzv.append(col)\n","    return nzv\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def analyze_file(path, outdir, label=\"file\"):\n","    df = pd.read_csv(path)\n","    name = os.path.splitext(os.path.basename(path))[0]\n","    prefix = f\"{label}_{name}\"\n","    info = {}\n","    info['path'] = path\n","    info['name'] = name\n","    info['shape'] = [int(df.shape[0]), int(df.shape[1])]\n","    info['summary'] = summarize_df_basic(df)\n","\n","    mkdir_p(outdir)\n","    missing_col_png = os.path.join(outdir, f\"{safe_filename(prefix)}_missing_by_column.png\")\n","    m1 = plot_missing_by_column(df, missing_col_png, title_suffix=f\"({name})\")\n","    if m1:\n","        info.setdefault('plots', []).append(m1)\n","\n","    missing_matrix_png = os.path.join(outdir, f\"{safe_filename(prefix)}_missing_matrix.png\")\n","    m2 = plot_missing_matrix(df, missing_matrix_png, title_suffix=f\"({name})\")\n","    if m2:\n","        info.setdefault('plots', []).append(m2)\n","\n","    missing_row_png = os.path.join(outdir, f\"{safe_filename(prefix)}_missing_by_row.png\")\n","    m3 = plot_missing_row_counts(df, missing_row_png, title_suffix=f\"({name})\")\n","    if m3:\n","        info.setdefault('plots', []).append(m3)\n","\n","    dtype_png = os.path.join(outdir, f\"{safe_filename(prefix)}_dtypes.png\")\n","    m4 = plot_dtype_counts(df, dtype_png, title_suffix=f\"({name})\")\n","    if m4:\n","        info.setdefault('plots', []).append(m4)\n","\n","    dist_files = plot_numeric_distributions(df, outdir, prefix, max_cols=12)\n","    info.setdefault('plots', []).extend(dist_files)\n","\n","    top_corr_pairs, corr_df = compute_top_correlations(df, top_k=30)\n","    info['top_correlations'] = [{'col1': a, 'col2': b, 'corr': c} for (a,b,c) in top_corr_pairs]\n","    corr_png = os.path.join(outdir, f\"{safe_filename(prefix)}_correlation_heatmap.png\")\n","    m5 = plot_correlation_heatmap(corr_df, corr_png, title_suffix=f\"({name})\")\n","    if m5:\n","        info.setdefault('plots', []).append(m5)\n","\n","    cat_summary, cat_plots = analyze_categorical(df, outdir, prefix, max_unique_for_plot=30)\n","    info['categorical_summary'] = cat_summary\n","    info.setdefault('plots', []).extend(cat_plots)\n","\n","    nzv = find_near_zero_variance(df)\n","    info['near_zero_variance_candidates'] = nzv\n","\n","    labels = detect_label_columns(df)\n","    info['label_candidates'] = labels\n","    if labels:\n","        info['label_balance'] = {}\n","        for label_col in labels:\n","            counts = df[label_col].value_counts(dropna=False).to_dict()\n","            info['label_balance'][label_col] = {str(k): int(v) for k, v in counts.items()}\n","\n","    info['top_missing_columns'] = df.isna().sum().sort_values(ascending=False).head(20).to_dict()\n","\n","    return df, info\n","\n","def compare_two_frames(df1, df2):\n","    comp = {}\n","    comp['shape_left'] = [int(df1.shape[0]), int(df1.shape[1])]\n","    comp['shape_right'] = [int(df2.shape[0]), int(df2.shape[1])]\n","    cols1 = set(df1.columns)\n","    cols2 = set(df2.columns)\n","    comp['common_columns'] = sorted(list(cols1 & cols2))\n","    comp['left_only_columns'] = sorted(list(cols1 - cols2))\n","    comp['right_only_columns'] = sorted(list(cols2 - cols1))\n","    common = comp['common_columns']\n","    miss1 = df1[common].isna().sum()\n","    miss2 = df2[common].isna().sum()\n","    miss_comp = []\n","    for c in common:\n","        miss_comp.append({'column': c, 'missing_left': int(miss1[c]), 'missing_right': int(miss2[c])})\n","    comp['missing_comparison'] = sorted(miss_comp, key=lambda x: (abs(x['missing_left'] - x['missing_right'])), reverse=True)[:50]\n","    numeric_common = [c for c in common if pd.api.types.is_numeric_dtype(df1[c]) or pd.api.types.is_numeric_dtype(df2[c])]\n","    num_comp = []\n","    for c in numeric_common:\n","        s1 = df1[c].dropna()\n","        s2 = df2[c].dropna()\n","        if s1.empty or s2.empty:\n","            continue\n","        stats1 = {'mean': float(s1.mean()), 'std': float(s1.std()), 'median': float(s1.median())}\n","        stats2 = {'mean': float(s2.mean()), 'std': float(s2.std()), 'median': float(s2.median())}\n","        num_comp.append({'column': c, 'left': stats1, 'right': stats2})\n","    comp['numeric_stats_comparison'] = num_comp[:200]\n","    return comp\n","\n","def save_summary_json(summary, outdir):\n","    timestamp = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n","    fname = os.path.join(outdir, f\"summary_{timestamp}.json\")\n","    with open(fname, \"w\", encoding=\"utf-8\") as f:\n","        json.dump(summary, f, indent=2)\n","    return fname"]},{"cell_type":"markdown","metadata":{},"source":["## Configuration\n","\n","Set the paths to your CSV files and output directory below."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["file1_path = \"BCCC-VolSCs-2023_Secure.csv\"\n","file2_path = \"BCCC-VolSCs-2023_Vulnerable.csv\"\n","outdir = \"analysis_output\"\n","sample = 0\n","show_plots = False"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mkdir_p(outdir)\n","\n","print(f\"Loading file1: {file1_path}\")\n","try:\n","    df1 = pd.read_csv(file1_path)\n","except Exception as e:\n","    print(f\"Failed to read {file1_path}: {e}\", file=sys.stderr)\n","    raise\n","print(f\"Loaded file1 shape: {df1.shape}\")\n","\n","print(f\"Loading file2: {file2_path}\")\n","try:\n","    df2 = pd.read_csv(file2_path)\n","except Exception as e:\n","    print(f\"Failed to read {file2_path}: {e}\", file=sys.stderr)\n","    raise\n","print(f\"Loaded file2 shape: {df2.shape}\")\n","\n","if sample and sample > 0:\n","    print(f\"Sampling up to {sample} rows from each file for lightweight analysis.\")\n","    df1 = df1.sample(n=min(sample, df1.shape[0]), random_state=0)\n","    df2 = df2.sample(n=min(sample, df2.shape[0]), random_state=0)\n","\n","print(\"Analyzing first file...\")\n","df1_full, info1 = analyze_file(file1_path, outdir, label=\"left\")\n","print(\"Analyzing second file...\")\n","df2_full, info2 = analyze_file(file2_path, outdir, label=\"right\")\n","\n","print(\"Comparing the two files...\")\n","comp = compare_two_frames(df1_full, df2_full)\n","\n","summary = {\n","    'analyzed_at_utc': datetime.utcnow().isoformat() + \"Z\",\n","    'file_left': info1,\n","    'file_right': info2,\n","    'comparison': comp\n","}\n","\n","summary_path = save_summary_json(summary, outdir)\n","print(f\"Saved JSON summary to: {summary_path}\")\n","\n","def print_basic_info(label, info):\n","    print(f\"\\n--- {label} ({info.get('name')}) ---\")\n","    print(f\"Path: {info.get('path')}\")\n","    print(f\"Shape: {info.get('shape')}\")\n","    print(f\"Memory usage: {info.get('summary', {}).get('memory_usage_human')}\")\n","    print(f\"Columns with missing values: {info.get('summary', {}).get('columns_missing_count')}\")\n","    print(f\"Rows with any missing values: {info.get('summary', {}).get('rows_with_any_missing')} ({info.get('summary', {}).get('percent_rows_with_missing')*100:.2f}%)\\")\n","    print(f\"Duplicate rows: {info.get('summary', {}).get('num_duplicates')}\")\n","    print(f\"Constant columns: {len(info.get('summary', {}).get('constant_columns', []))}\")\n","    if info.get('label_candidates'):\n","        print(f\"Potential label columns detected: {info.get('label_candidates')}\")\n","        for lc in info.get('label_candidates', []):\n","            print(f\"  Balance for {lc}: {info.get('label_balance', {}).get(lc)}\")\n","\n","print_basic_info(\"File 1\", info1)\n","print_basic_info(\"File 2\", info2)\n","\n","print(\"\\nTop differences in missingness between the two files (up to 50 shown):\")\n","for d in comp.get('missing_comparison', [])[:50]:\n","    print(f\"  {d['column']}: missing_left={d['missing_left']}, missing_right={d['missing_right']}\")\n","\n","print(\"\\nTop correlated numeric column pairs (file1):\")\n","for item in info1.get('top_correlations', [])[:10]:\n","    print(f\"  {item['col1']} <> {item['col2']}, corr={item['corr']:.3f}\")\n","\n","print(\"\\nTop correlated numeric column pairs (file2):\")\n","for item in info2.get('top_correlations', [])[:10]:\n","    print(f\"  {item['col1']} <> {item['col2']}, corr={item['corr']:.3f}\")\n","\n","print(f\"\\nAll charts and outputs saved to: {os.path.abspath(outdir)}\")\n","print(\"Done.\")"]}