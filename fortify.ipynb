{
 "cells": [
  {
   "cell_type": "code",
   "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vulnerable Smart Contracts Detection using ML/DL\n",
    "\n",
    "This notebook builds a binary classifier for Solidity smart contracts using:\n",
    "- **CodeBERT embeddings** for contract source code (microsoft/codebert-base)\n",
    "- **Bytecode-derived tabular features**\n",
    "- **Multiple MLP variants** with ensemble approach\n",
    "\n",
    "## Key Improvements:\n",
    "- ✓ **Embedding Caching**: CodeBERT embeddings cached to `/kaggle/working` for faster re-runs\n",
    "- ✓ **Reproducibility**: Random seeds set for deterministic behavior\n",
    "- ✓ **SWA (Stochastic Weight Averaging)**: Proper implementation with DataLoader\n",
    "- ✓ **Consistent Evaluation**: All models output logits; sigmoid applied only during evaluation\n",
    "- ✓ **Mini-batch Training**: All training uses DataLoader for scalability\n",
    "\n",
    "## Usage:\n",
    "- First run: Computes and caches embeddings\n",
    "- Subsequent runs: Loads embeddings from cache (much faster!)\n",
    "- To recompute: Delete `/kaggle/working/codebert_embeddings.npy`"
   ]
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\nsecure_df = pd.read_csv(\"/kaggle/input/bccc-vulscs-2023/BCCC-VolSCs-2023_Secure.csv\")\nvulnerable_df = pd.read_csv(\"/kaggle/input/bccc-vulscs-2023/BCCC-VolSCs-2023_Vulnerable.csv\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-01T13:05:38.585699Z",
     "iopub.execute_input": "2025-04-01T13:05:38.585989Z",
     "iopub.status.idle": "2025-04-01T13:05:39.567976Z",
     "shell.execute_reply.started": "2025-04-01T13:05:38.585968Z",
     "shell.execute_reply": "2025-04-01T13:05:39.567286Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "df = pd.concat([secure_df, vulnerable_df], ignore_index=True)\nprint(len(df))",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-01T13:05:57.54075Z",
     "iopub.execute_input": "2025-04-01T13:05:57.541029Z",
     "iopub.status.idle": "2025-04-01T13:05:57.569439Z",
     "shell.execute_reply.started": "2025-04-01T13:05:57.541009Z",
     "shell.execute_reply": "2025-04-01T13:05:57.568562Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "contract_codes = []\nfor hash_id in df['hash_id']:\n    file_path = f\"/kaggle/input/contractcodes/source/{hash_id}.sol\"  \n    with open(file_path, 'r', encoding='utf-8') as file:\n        contract_codes.append(file.read())\n\ndf['contract_code'] = contract_codes",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-01T13:17:24.406775Z",
     "iopub.execute_input": "2025-04-01T13:17:24.407077Z",
     "iopub.status.idle": "2025-04-01T13:17:45.030445Z",
     "shell.execute_reply.started": "2025-04-01T13:17:24.407053Z",
     "shell.execute_reply": "2025-04-01T13:17:45.029581Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "print(df.head())",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-01T13:18:03.797422Z",
     "iopub.execute_input": "2025-04-01T13:18:03.797764Z",
     "iopub.status.idle": "2025-04-01T13:18:03.823201Z",
     "shell.execute_reply.started": "2025-04-01T13:18:03.797733Z",
     "shell.execute_reply": "2025-04-01T13:18:03.822359Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set random seeds for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"✓ Random seed set to {seed} for reproducibility\")\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "source": "import torch\nprint(torch.__version__)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-01T13:18:11.10701Z",
     "iopub.execute_input": "2025-04-01T13:18:11.107307Z",
     "iopub.status.idle": "2025-04-01T13:18:14.342596Z",
     "shell.execute_reply.started": "2025-04-01T13:18:11.107285Z",
     "shell.execute_reply": "2025-04-01T13:18:14.341689Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import os\n",
    "\n",
    "def make_loaders(X_train, y_train, X_val, y_val, batch_size=128):\n",
    "    \"\"\"Create DataLoaders for training and validation\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Convert to tensors if needed\n",
    "    if not isinstance(X_train, torch.Tensor):\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    if not isinstance(y_train, torch.Tensor):\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "    if not isinstance(X_val, torch.Tensor):\n",
    "        X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "    if not isinstance(y_val, torch.Tensor):\n",
    "        y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "    \n",
    "    # Ensure y is 1D for TensorDataset\n",
    "    if y_train.dim() > 1:\n",
    "        y_train = y_train.squeeze()\n",
    "    if y_val.dim() > 1:\n",
    "        y_val = y_val.squeeze()\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    \n",
    "    print(f\"✓ Created DataLoaders - batch_size={batch_size}, train_batches={len(train_loader)}, val_batches={len(val_loader)}\")\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def compute_codebert_embeddings(df, code_col=\"contract_code\", cache_dir=\"/kaggle/working\", batch_size=32):\n",
    "    \"\"\"Compute or load CodeBERT embeddings with caching\"\"\"\n",
    "    from transformers import RobertaTokenizer, RobertaModel\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    cache_embeddings = os.path.join(cache_dir, \"codebert_embeddings.npy\")\n",
    "    cache_ids = os.path.join(cache_dir, \"ids.npy\")\n",
    "    \n",
    "    # Check cache\n",
    "    if os.path.exists(cache_embeddings) and os.path.exists(cache_ids):\n",
    "        print(\"Checking cache...\")\n",
    "        try:\n",
    "            embeddings = np.load(cache_embeddings)\n",
    "            cached_ids = np.load(cache_ids)\n",
    "            \n",
    "            # Verify match\n",
    "            if len(embeddings) == len(df) and np.array_equal(cached_ids, df.index.values):\n",
    "                print(f\"✓ Loaded embeddings from cache: shape={embeddings.shape}, dtype={embeddings.dtype}\")\n",
    "                return embeddings\n",
    "            else:\n",
    "                print(\"Cache mismatch - recomputing...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Cache load error: {e} - recomputing...\")\n",
    "    \n",
    "    # Compute embeddings\n",
    "    print(\"Computing CodeBERT embeddings...\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\", use_fast=True)\n",
    "    model = RobertaModel.from_pretrained(\"microsoft/codebert-base\").to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    def batch_embed(texts):\n",
    "        inputs = tokenizer(texts, return_tensors=\"pt\", max_length=512, \n",
    "                          truncation=True, padding=\"max_length\").to(device)\n",
    "        with torch.no_grad():\n",
    "            if device.type == \"cuda\":\n",
    "                with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "                    outputs = model(**inputs)\n",
    "            else:\n",
    "                outputs = model(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1).cpu().numpy().astype(np.float32)\n",
    "    \n",
    "    # Process in batches\n",
    "    all_embeddings = []\n",
    "    contracts = df[code_col].tolist()\n",
    "    \n",
    "    for i in tqdm(range(0, len(contracts), batch_size), desc=\"Embedding batches\"):\n",
    "        batch = contracts[i:i + batch_size]\n",
    "        all_embeddings.extend(batch_embed(batch))\n",
    "    \n",
    "    embeddings = np.array(all_embeddings, dtype=np.float32)\n",
    "    \n",
    "    # Save cache\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    np.save(cache_embeddings, embeddings)\n",
    "    np.save(cache_ids, df.index.values)\n",
    "    print(f\"✓ Saved to cache: shape={embeddings.shape}, dtype={embeddings.dtype}\")\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute CodeBERT embeddings with caching\n",
    "EMBEDDING_BATCH_SIZE = 32  # Adjust based on GPU memory\n",
    "\n",
    "embeddings_array = compute_codebert_embeddings(\n",
    "    df, \n",
    "    code_col=\"contract_code\",\n",
    "    cache_dir=\"/kaggle/working\",\n",
    "    batch_size=EMBEDDING_BATCH_SIZE\n",
    ")\n",
    "\n",
    "df[\"code_embedding\"] = list(embeddings_array)\n",
    "print(f\"\\nEmbeddings shape: {embeddings_array.shape}\")\n",
    "print(f\"Embeddings dtype: {embeddings_array.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select bytecode features\n",
    "bytecode_features = df[[\"Weight bytecode_character_6\", \"Weight bytecode_character_0\", \n",
    "                         \"Weight bytecode_character_8\", \"Weight bytecode_character_4\", \n",
    "                         \"Weight bytecode_character_5\", \"Weight bytecode_character_2\"]]\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "bytecode_features_scaled = scaler.fit_transform(bytecode_features)\n",
    "\n",
    "# Combine embeddings + bytecode\n",
    "X = np.hstack([np.stack(df[\"code_embedding\"].values), bytecode_features_scaled])\n",
    "y = df[\"label\"].values\n",
    "\n",
    "print(f\"Feature matrix: {X.shape}\")\n",
    "print(f\"Labels: {y.shape}, distribution: {np.unique(y, return_counts=True)}\")\n",
    "\n",
    "# Single train/val split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Train: {X_train.shape[0]} samples, Val: {X_val.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class CodeBERTClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(CodeBERTClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 2048)\n",
    "        self.bn1 = nn.BatchNorm1d(2048)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(2048, 1024)\n",
    "        self.bn2 = nn.BatchNorm1d(1024)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc3 = nn.Linear(1024, 512)\n",
    "        self.bn3 = nn.BatchNorm1d(512)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        self.fc4 = nn.Linear(512, 256)\n",
    "        self.bn4 = nn.BatchNorm1d(256)\n",
    "        self.dropout4 = nn.Dropout(0.3)\n",
    "        self.fc5 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(self.bn1(torch.relu(self.fc1(x))))\n",
    "        x = self.dropout2(self.bn2(torch.relu(self.fc2(x))))\n",
    "        x = self.dropout3(self.bn3(torch.relu(self.fc3(x))))\n",
    "        x = self.dropout4(self.bn4(torch.relu(self.fc4(x))))\n",
    "        x = self.fc5(x)  # Return logits (no sigmoid)\n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "model = CodeBERTClassifier(input_dim=X_train.shape[1]).to(device)\n",
    "\n",
    "# Use BCEWithLogitsLoss (combines sigmoid + BCE for numerical stability)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-6)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.001)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader, val_loader = make_loaders(X_train, y_train, X_val, y_val, batch_size=128)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 250\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.unsqueeze(1).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    scheduler.step()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.unsqueeze(1).to(device)\n",
    "            \n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation - apply sigmoid to get probabilities\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n",
    "\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Get predictions for validation set\n",
    "all_logits = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in val_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        logits = model(batch_X)\n",
    "        all_logits.append(logits.cpu())\n",
    "        all_labels.append(batch_y.cpu())\n",
    "\n",
    "# Concatenate all batches\n",
    "val_logits = torch.cat(all_logits, dim=0)\n",
    "val_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "# Apply sigmoid to get probabilities\n",
    "val_probs = torch.sigmoid(val_logits).numpy()\n",
    "val_preds = (val_probs > 0.5).astype(float)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = (val_preds.squeeze() == val_labels.numpy()).mean()\n",
    "precision = precision_score(val_labels.numpy(), val_preds)\n",
    "recall = recall_score(val_labels.numpy(), val_preds)\n",
    "f1 = f1_score(val_labels.numpy(), val_preds)\n",
    "cm = confusion_matrix(val_labels.numpy(), val_preds)\n",
    "\n",
    "print(\"\\n=== Validation Metrics ===\")\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-Score:  {f1:.4f}\")\n",
    "print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "\n",
    "# ROC-AUC\n",
    "fpr, tpr, _ = roc_curve(val_labels.numpy(), val_probs)\n",
    "auc_score = auc(fpr, tpr)\n",
    "print(f\"\\nAUC: {auc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "prob_true, prob_pred = calibration_curve(val_labels.numpy(), val_probs.squeeze(), n_bins=10)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(prob_pred, prob_true, marker='o', label=\"CodeBERT Classifier\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", label=\"Perfectly calibrated\")\n",
    "plt.xlabel(\"Mean predicted value\")\n",
    "plt.ylabel(\"Fraction of positives\")\n",
    "plt.title(\"Calibration Curve\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = nn.BCEWithLogitsLoss(reduction='none')(inputs, targets)\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * bce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "class TemperatureScaling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TemperatureScaling, self).__init__()\n",
    "        self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
    "        \n",
    "    def forward(self, logits):\n",
    "        return logits / self.temperature\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.fc = nn.Linear(in_features, out_features)\n",
    "        self.bn = nn.BatchNorm1d(out_features)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.adapter = nn.Linear(in_features, out_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = torch.relu(self.fc(x))\n",
    "        out = self.bn(out)\n",
    "        out = self.dropout(out)\n",
    "        return out + self.adapter(identity)\n",
    "\n",
    "class ImprovedCodeBERTClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ImprovedCodeBERTClassifier, self).__init__()\n",
    "        # Initial layer\n",
    "        self.fc1 = nn.Linear(input_dim, 2048)\n",
    "        self.bn1 = nn.BatchNorm1d(2048)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.res1 = ResidualBlock(2048, 1024)\n",
    "        self.res2 = ResidualBlock(1024, 512)\n",
    "        self.res3 = ResidualBlock(512, 256)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc_out = nn.Linear(256, 1)\n",
    "        \n",
    "        # Temperature scaling for calibration\n",
    "        self.temperature = TemperatureScaling()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initial layer\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Residual blocks\n",
    "        x = self.res1(x)\n",
    "        x = self.res2(x)\n",
    "        x = self.res3(x)\n",
    "        \n",
    "        # Output layer (logits)\n",
    "        x = self.fc_out(x)\n",
    "        \n",
    "        # Apply temperature scaling (still logits)\n",
    "        x = self.temperature(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        return max(0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps)))\n",
    "    \n",
    "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# Initialize model\n",
    "model = ImprovedCodeBERTClassifier(input_dim=X_train.shape[1]).to(device)\n",
    "criterion = FocalLoss(alpha=0.25, gamma=2.0)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=5e-5)\n",
    "\n",
    "# Scheduler\n",
    "total_epochs = 350\n",
    "warmup_steps = 15\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_epochs)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader, val_loader = make_loaders(X_train, y_train, X_val, y_val, batch_size=128)\n",
    "\n",
    "# Training with early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 20\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.unsqueeze(1).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    scheduler.step()\n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.unsqueeze(1).to(device)\n",
    "            \n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            correct += (preds == batch_y).sum().item()\n",
    "            total += batch_y.size(0)\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = correct / total\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{total_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"✓ New best model saved!\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# Load best model\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(\"\\n✓ Loaded best model\")\n",
    "\n",
    "# Final evaluation\n",
    "model.eval()\n",
    "all_logits = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in val_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        logits = model(batch_X)\n",
    "        all_logits.append(logits.cpu())\n",
    "        all_labels.append(batch_y.cpu())\n",
    "\n",
    "val_logits = torch.cat(all_logits, dim=0)\n",
    "val_labels = torch.cat(all_labels, dim=0)\n",
    "val_probs = torch.sigmoid(val_logits).numpy()\n",
    "\n",
    "print(f\"\\nFinal Validation Accuracy: {(val_probs.squeeze() > 0.5).astype(float).mean():.4f}\")\n",
    "\n",
    "# Plot calibration\n",
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "prob_true, prob_pred = calibration_curve(val_labels.numpy(), val_probs.squeeze(), n_bins=10)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "ax2 = plt.subplot2grid((3, 1), (2, 0))\n",
    "\n",
    "ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "ax1.plot(prob_pred, prob_true, \"s-\", label=f\"Improved Model\")\n",
    "ax1.set_ylabel(\"Fraction of positives\")\n",
    "ax1.set_ylim([-0.05, 1.05])\n",
    "ax1.set_title(\"Calibration Curve\")\n",
    "ax1.legend(loc=\"lower right\")\n",
    "\n",
    "ax2.hist(val_probs.squeeze(), range=(0, 1), bins=10, histtype=\"step\", lw=2)\n",
    "ax2.set_xlabel(\"Mean predicted value\")\n",
    "ax2.set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Feature dimensionality reduction\n",
    "def reduce_dimensions(X_train, X_val, n_components=150):\n",
    "    print(f\"Original feature dimensions: {X_train.shape[1]}\")\n",
    "    \n",
    "    # Standardize\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    # PCA\n",
    "    pca = PCA(n_components=n_components, random_state=42)\n",
    "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "    X_val_pca = pca.transform(X_val_scaled)\n",
    "    \n",
    "    explained_var = np.sum(pca.explained_variance_ratio_)\n",
    "    print(f\"Reduced to {n_components} components, explaining {explained_var:.4f} of variance\")\n",
    "    \n",
    "    return X_train_pca, X_val_pca, pca, scaler\n",
    "\n",
    "# Mixup data augmentation\n",
    "def mixup_data(x, y, alpha=0.2):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "# Base model with regularization (logits output)\n",
    "class RegularizedModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[512, 256, 128], dropout_rate=0.5):\n",
    "        super(RegularizedModel, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Input layer\n",
    "        self.layers.append(nn.Linear(input_dim, hidden_dims[0]))\n",
    "        self.layers.append(nn.BatchNorm1d(hidden_dims[0]))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Dropout(dropout_rate))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            self.layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            self.layers.append(nn.BatchNorm1d(hidden_dims[i+1]))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Dropout(dropout_rate))\n",
    "        \n",
    "        # Output layer (logits)\n",
    "        self.output = nn.Linear(hidden_dims[-1], 1)\n",
    "        \n",
    "        self.l2_reg = 1e-4\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.output(x)  # Return logits\n",
    "        \n",
    "        # L2 regularization\n",
    "        l2_loss = 0.0\n",
    "        for param in self.parameters():\n",
    "            l2_loss += torch.norm(param)\n",
    "        self.l2_loss = self.l2_reg * l2_loss\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_l2_loss(self):\n",
    "        return self.l2_loss\n",
    "\n",
    "# Different architectures for ensemble\n",
    "class WideModel(RegularizedModel):\n",
    "    def __init__(self, input_dim):\n",
    "        super(WideModel, self).__init__(input_dim, hidden_dims=[1024, 512, 256], dropout_rate=0.5)\n",
    "\n",
    "class DeepModel(RegularizedModel):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DeepModel, self).__init__(input_dim, hidden_dims=[512, 256, 128, 64], dropout_rate=0.4)\n",
    "\n",
    "class CompactModel(RegularizedModel):\n",
    "    def __init__(self, input_dim):\n",
    "        super(CompactModel, self).__init__(input_dim, hidden_dims=[256, 128], dropout_rate=0.3)\n",
    "\n",
    "# Ensemble wrapper\n",
    "class EnsembleModel:\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    def predict(self, x):\n",
    "        predictions = []\n",
    "        for model in self.models:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                logits = model(x)\n",
    "                probs = torch.sigmoid(logits)\n",
    "                predictions.append(probs)\n",
    "        \n",
    "        # Average predictions\n",
    "        return torch.stack(predictions).mean(dim=0)\n",
    "\n",
    "# Training function with fixed SWA and DataLoader\n",
    "def train_model(model, X_train, y_train, X_val, y_val, criterion, optimizer, \n",
    "                scheduler=None, epochs=200, batch_size=128, \n",
    "                patience=25, use_mixup=True, alpha=0.2, use_swa=True):\n",
    "    \n",
    "    # Create DataLoaders (FIXED: was missing before)\n",
    "    train_loader, val_loader = make_loaders(X_train, y_train, X_val, y_val, batch_size=batch_size)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    # SWA setup\n",
    "    swa_start = epochs // 2\n",
    "    swa_model = None\n",
    "    swa_scheduler = None\n",
    "    \n",
    "    if use_swa:\n",
    "        swa_model = torch.optim.swa_utils.AveragedModel(model)\n",
    "        swa_scheduler = torch.optim.swa_utils.SWALR(optimizer, swa_lr=0.0005)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.unsqueeze(1).to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Apply mixup if enabled\n",
    "            if use_mixup and epoch < epochs * 0.8:\n",
    "                batch_X, targets_a, targets_b, lam = mixup_data(batch_X, batch_y, alpha)\n",
    "                outputs = model(batch_X)\n",
    "                loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "            else:\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            # Add L2 regularization if available\n",
    "            if hasattr(model, 'get_l2_loss'):\n",
    "                loss += model.get_l2_loss()\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        epoch_loss /= len(train_loader)\n",
    "        \n",
    "        # Update SWA after swa_start\n",
    "        if use_swa and epoch >= swa_start:\n",
    "            swa_model.update_parameters(model)\n",
    "            swa_scheduler.step()\n",
    "        elif scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X = batch_X.to(device)\n",
    "                batch_y = batch_y.unsqueeze(1).to(device)\n",
    "                \n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                correct += (preds == batch_y).sum().item()\n",
    "                total += batch_y.size(0)\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = correct / total\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(\"✓ Loaded best model\")\n",
    "    \n",
    "    # Finalize SWA model (FIXED: use train_loader instead of undefined X_train_loader)\n",
    "    if use_swa and swa_model is not None and epoch >= swa_start:\n",
    "        print(\"Updating batch normalization for SWA model...\")\n",
    "        torch.optim.swa_utils.update_bn(train_loader, swa_model)\n",
    "        \n",
    "        # Evaluate SWA model\n",
    "        swa_model.eval()\n",
    "        swa_correct = 0\n",
    "        swa_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X = batch_X.to(device)\n",
    "                batch_y = batch_y.unsqueeze(1).to(device)\n",
    "                \n",
    "                outputs = swa_model(batch_X)\n",
    "                preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                swa_correct += (preds == batch_y).sum().item()\n",
    "                swa_total += batch_y.size(0)\n",
    "        \n",
    "        swa_accuracy = swa_correct / swa_total\n",
    "        print(f\"SWA Model Val Accuracy: {swa_accuracy:.4f}\")\n",
    "        \n",
    "        # Use SWA model if better\n",
    "        if swa_accuracy > val_accuracy:\n",
    "            print(\"✓ Using SWA model (better performance)\")\n",
    "            # Copy SWA parameters to model\n",
    "            model.load_state_dict(swa_model.module.state_dict())\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Main ensemble training\n",
    "def run_ensemble_training(X_train, y_train, X_val, y_val):\n",
    "    # Reduce dimensions\n",
    "    X_train_reduced, X_val_reduced, pca, scaler = reduce_dimensions(X_train, X_val, n_components=150)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Train different models for ensemble\n",
    "    input_dim = X_train_reduced.shape[1]\n",
    "    models = []\n",
    "    \n",
    "    # Model 1: Wide architecture\n",
    "    print(\"\\n=== Training Wide Model ===\")\n",
    "    model1 = WideModel(input_dim).to(device)\n",
    "    optimizer1 = optim.AdamW(model1.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    scheduler1 = optim.lr_scheduler.CosineAnnealingLR(optimizer1, T_max=100, eta_min=1e-5)\n",
    "    model1 = train_model(model1, X_train_reduced, y_train, X_val_reduced, y_val, \n",
    "                        criterion, optimizer1, scheduler1, epochs=200, batch_size=128, \n",
    "                        patience=25, use_mixup=True, alpha=0.2, use_swa=True)\n",
    "    models.append(model1)\n",
    "    \n",
    "    # Model 2: Deep architecture\n",
    "    print(\"\\n=== Training Deep Model ===\")\n",
    "    model2 = DeepModel(input_dim).to(device)\n",
    "    optimizer2 = optim.AdamW(model2.parameters(), lr=0.002, weight_decay=1e-5)\n",
    "    scheduler2 = optim.lr_scheduler.CosineAnnealingLR(optimizer2, T_max=100, eta_min=1e-5)\n",
    "    model2 = train_model(model2, X_train_reduced, y_train, X_val_reduced, y_val, \n",
    "                        criterion, optimizer2, scheduler2, epochs=200, batch_size=64, \n",
    "                        patience=25, use_mixup=True, alpha=0.3, use_swa=True)\n",
    "    models.append(model2)\n",
    "    \n",
    "    # Model 3: Compact architecture\n",
    "    print(\"\\n=== Training Compact Model ===\")\n",
    "    model3 = CompactModel(input_dim).to(device)\n",
    "    optimizer3 = optim.Adam(model3.parameters(), lr=0.003, weight_decay=1e-6)\n",
    "    scheduler3 = optim.lr_scheduler.StepLR(optimizer3, step_size=30, gamma=0.5)\n",
    "    model3 = train_model(model3, X_train_reduced, y_train, X_val_reduced, y_val, \n",
    "                        criterion, optimizer3, scheduler3, epochs=200, batch_size=256, \n",
    "                        patience=25, use_mixup=False, use_swa=False)\n",
    "    models.append(model3)\n",
    "    \n",
    "    # Create ensemble\n",
    "    ensemble = EnsembleModel(models)\n",
    "    \n",
    "    # Evaluate individual models and ensemble\n",
    "    X_val_torch = torch.tensor(X_val_reduced, dtype=torch.float32).to(device)\n",
    "    y_val_torch = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    \n",
    "    print(\"\\n=== Individual Model Performance ===\")\n",
    "    for i, model in enumerate(models):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model(X_val_torch)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > 0.5).float()\n",
    "            acc = (preds == y_val_torch).float().mean().item()\n",
    "        print(f\"Model {i+1} Val Accuracy: {acc:.4f}\")\n",
    "    \n",
    "    # Ensemble evaluation\n",
    "    ensemble_probs = ensemble.predict(X_val_torch)\n",
    "    ensemble_preds = (ensemble_probs > 0.5).float()\n",
    "    ensemble_acc = (ensemble_preds == y_val_torch).float().mean().item()\n",
    "    \n",
    "    print(f\"\\n✓ Ensemble Val Accuracy: {ensemble_acc:.4f}\")\n",
    "    \n",
    "    # Plot calibration for ensemble\n",
    "    val_probs_np = ensemble_probs.cpu().numpy()\n",
    "    \n",
    "    prob_true, prob_pred = calibration_curve(y_val, val_probs_np.squeeze(), n_bins=10)\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "    ax2 = plt.subplot2grid((3, 1), (2, 0))\n",
    "    \n",
    "    ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "    ax1.plot(prob_pred, prob_true, \"s-\", label=f\"Ensemble (Acc: {ensemble_acc:.3f})\")\n",
    "    ax1.set_ylabel(\"Fraction of positives\")\n",
    "    ax1.set_ylim([-0.05, 1.05])\n",
    "    ax1.set_title(\"Ensemble Calibration Curve\")\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "    \n",
    "    ax2.hist(val_probs_np.squeeze(), range=(0, 1), bins=10, histtype=\"step\", lw=2)\n",
    "    ax2.set_xlabel(\"Mean predicted value\")\n",
    "    ax2.set_ylabel(\"Count\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return ensemble, models, pca, scaler\n",
    "\n",
    "# Run ensemble training\n",
    "ensemble, models, pca, scaler = run_ensemble_training(X_train, y_train, X_val, y_val)\n",
    "\n",
    "print(\"\\n✓ All training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Summary\n",
    "\n",
    "All models trained successfully with the following improvements:\n",
    "\n",
    "### Fixed Issues:\n",
    "1. ✅ **SWA update_bn crash**: Now uses proper `train_loader` instead of undefined `X_train_loader`\n",
    "2. ✅ **Batching**: All training uses DataLoader with mini-batches\n",
    "3. ✅ **Loss consistency**: All models output logits + use BCEWithLogitsLoss\n",
    "4. ✅ **Single split**: Removed duplicate test split, using consistent train/val split\n",
    "5. ✅ **Reproducibility**: Seeds set at the beginning\n",
    "6. ✅ **Embedding caching**: Embeddings cached to `/kaggle/working` for fast re-runs\n",
    "\n",
    "### Key Features:\n",
    "- **CodeBERT Embeddings**: Cached for efficiency\n",
    "- **DataLoader Training**: Proper mini-batch training for all models\n",
    "- **Logits + BCEWithLogitsLoss**: Consistent across all models\n",
    "- **Sigmoid in Evaluation Only**: Applied explicitly when computing metrics\n",
    "- **SWA Integration**: Properly implemented with DataLoader\n",
    "- **Ensemble Learning**: Multiple architectures combined for robust predictions\n",
    "\n",
    "### Metrics Available:\n",
    "- Accuracy, Precision, Recall, F1-Score\n",
    "- Confusion Matrix\n",
    "- ROC-AUC\n",
    "- Calibration Curves"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 8500769,
     "sourceType": "datasetVersion",
     "datasetId": 5073272
    },
    {
     "sourceId": 11240610,
     "sourceType": "datasetVersion",
     "datasetId": 7022778
    }
   ],
   "dockerImageVersionId": 30918,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}